Linear Regression

Linear regression is like drawing the best possible straight line (or plane) through the data so we can predict outcomes.
Equation looks like:

ğ‘¦
=
ğ›½
0
+
ğ›½
1
ğ‘¥
1
+
ğ›½
2
ğ‘¥
2
+
.
.
.
+
ğ›½
ğ‘›
ğ‘¥
ğ‘›
+
ğœ–
y=Î²
0
	â€‹

+Î²
1
	â€‹

x
1
	â€‹

+Î²
2
	â€‹

x
2
	â€‹

+...+Î²
n
	â€‹

x
n
	â€‹

+Ïµ

ğ‘¦
y â†’ the value we want to predict

ğ‘¥
xâ€™s â†’ features (inputs)

ğ›½
Î²â€™s â†’ weights/coefficients (how important each feature is)

ğœ–
Ïµ â†’ error

The goal is to minimize the error (difference between predicted and actual).

ğŸ‘‰ Works well for simple data, butâ€¦ if features are correlated or there are too many, it can overfit.

ğŸ”¹ Regularization â€“ Fix for Overfitting

Regularization = adding a penalty to the model so it doesnâ€™t rely too heavily on certain features.

Two main types:

1. Ridge Regression (L2 Regularization)

Adds a penalty based on the square of coefficients.

It makes coefficients smaller but never exactly zero.

Good when all features are somewhat useful.

Think of it like: â€œKeep everything, but control the weight.â€

Equation:

ğ¿
ğ‘œ
ğ‘ 
ğ‘ 
=
ğ‘€
ğ‘†
ğ¸
+
ğœ†
âˆ‘
ğ›½
ğ‘—
2
Loss=MSE+Î»âˆ‘Î²
j
2
	â€‹

2. Lasso Regression (L1 Regularization)

Adds a penalty based on the absolute value of coefficients.

Pushes some coefficients all the way to zero â†’ automatic feature selection.

Best when you have many features and only some really matter.

Equation:

ğ¿
ğ‘œ
ğ‘ 
ğ‘ 
=
ğ‘€
ğ‘†
ğ¸
+
ğœ†
âˆ‘
âˆ£
ğ›½
ğ‘—
âˆ£
Loss=MSE+Î»âˆ‘âˆ£Î²
j
	â€‹

âˆ£
